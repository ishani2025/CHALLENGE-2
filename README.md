# ğŸ¤–   LogSentiel

<div align="center"> <b>LogSentinel</b> <p><strong>Dual-Agent AI System for Enterprise IT Troubleshooting</strong></p> <p> <img src="https://img.shields.io/badge/Python-3.8+-blue" alt="Python Version"/> <img src="https://img.shields.io/badge/LLM-Ollama%20(Mistral)-green" alt="LLM"/> <img src="https://img.shields.io/badge/API-ScaleDown-orange" alt="ScaleDown API"/> <img src="https://img.shields.io/badge/License-MIT-yellow" alt="License"/> <img src="https://img.shields.io/badge/Status-Active-success" alt="Status"/> </p> </div>

## ğŸŒŸ Overview
LogSentinel is an enterprise-grade troubleshooting assistant that analyzes large error logs (10,000+ lines), retrieves similar historical issues, generates structured step-by-step solutions, and intelligently escalates low-confidence cases.

It combines:
   - ScaleDown API for log compression
   - Local LLM (Ollama + Mistral) for reasoning
   - Embedding-based similarity search
   - Confidence-driven escalation workflow
   - Optional Jira integration

### ğŸ¯ Key Features

- ğŸ§  Dual-Agent Architecture (Log Analyst + Solution Engineer)
- ğŸ’­Log Compression (~85% reduction via ScaleDown)
- ğŸ” Semantic Similarity Search (StackOverflow, GitHub, Runbooks)
- ğŸ“Š Confidence-Based Decision Engine
- ğŸ”„ Automated Escalation to Jira
- ğŸ  Local LLM Processing (Sensitive logs stay local)
- ğŸ“ˆ MTTR Tracking & Observability

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Ollama installed
- Mistral model pulled
- ScaleDown API Key
- Optional: Jira API credentials

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/ishani2025/CHALLENGE-2.git
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install & Start Ollama**
   ```bash 
   ollama pull mistral
   ollama run  mistral
   ```

4. **Configure Environment Variables**
   ```bash
   Create a .env file and in that have:
   SCALEDOWN_API_KEY=your_key
   SCALEDOWN_BASE_URL=your_url
   JIRA_API_KEY=your_jira_key
   ```



## ğŸ—ï¸ Architecture

```
LogSentiel
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ agent1.py
â”‚   â”‚   â”œâ”€â”€ agent2.py
â”‚   â”‚------llm/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ __init__t.py     
â”‚   â”‚   â”œâ”€â”€ embedder.py       
â”‚   â”‚   â””â”€â”€ vector_store.py    
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ file_utils.py        
â”‚   â”‚   â””â”€â”€ logger.py       
â”‚   â””â”€â”€ decision/
â”‚       â””â”€â”€ decision_controller.py    
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€compressed_logs/              
â”‚   â””â”€â”€ raw_logs/            
â”œâ”€â”€ tests/
â”œâ”€â”€assessts/
â””â”€â”€ config/
â”œâ”€â”€ requirements.txt
â””â”€â”€ app.py
```

## ğŸ¤– Dual-Agent Logic

### Agent 1 â€“ Log Analyst
- Calls ScaleDown API
- Reduces noise
- Extracts primary error signals
- Categorizes failure type
### Agent 2 â€“ Solution Engineer
- Retrieves similar historical cases
- Generates grounded resolution steps
- Computes confidence score

## ğŸ’¬ Confidence Thresholds
```bash 
   Score     | Action         
   | ------- | -------------- |
   | â‰¥ 0.85  | Auto Resolve   |
   | 0.65â€“0.85 | Suggest Review |
   | < 0.65  | Escalate       |
```
## ğŸ“Š Metrics & Observability

The system tracks:
- Compression Ratio
- Resolution Success Rate
- Escalation Frequency
- Estimated MTTR Reduction
## ğŸ”§ Running the Application
CLI Mode:
```bash
python app.py
```
Web Interface:
```bash
streamlit run ui/web_app.py
```
## ğŸ“ˆ Benefits
- Reduces log size by up to 85%
- Accelerates troubleshooting workflows
- Minimizes unnecessary escalations
- Keeps sensitive data local
- Structured enterprise-aligned pipeline

## âš ï¸ Limitations

- Depends on knowledge base quality
- Confidence scoring requires tuning
- Novel errors may not match historical cases
- Not a full ITSM replacement
- Relies on ScaleDown API availability

ğŸ§ª
---
